# -*- coding: utf-8 -*-
"""Employee_Performance_Rating_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WX5TdtOQ_WbJ1Rrww0kcfrtsYGRjO54b

# Importing Required Libraries
"""

#importing relevant libraries
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
le= LabelEncoder()

"""# Importing the HR Dataset"""

#Import Data
Emp_Perf=pd.read_excel("Hr_data.xls") # Dataset used
Emp_Perf.head(5)

"""# Exploratory Data Analysis

## Checking Datatypes, Count and Null Values in Dataset
"""

Emp_Perf.info()

"""## Number of Rows and Columns in Dataset"""

Emp_Perf.shape

"""## Check for any missing / null value in the data"""

Emp_Perf.isna().sum()

"""## Dropping Duplicated Values"""

Emp_Perf[Emp_Perf.duplicated()]

"""# Visualizing some Categorical Varibles

## Visualizing on Education Background
"""

#Analyze Educational Background
sns.set(rc={'figure.figsize':(7,7)})
sns.countplot(x="EducationBackground", data=Emp_Perf)

"""## Distribution of Employee's at Department Level"""

# Distribution of the employees in the various departments:
Emp_Perf['EmpDepartment'].value_counts()

"""## Department Wise Employee Performance Ratings"""

# Department-wise performance rating of the employees:
Emp_Perf.groupby("EmpDepartment")['PerformanceRating'].mean()

"""## Visualizing Gender count in Dataset based on Performance Ratings"""

print("Total Headcount in the dataset: " + str(len(Emp_Perf.index)))
#Analyse the data
sns.countplot(x="PerformanceRating", hue= "Gender" , data=Emp_Perf)

"""## Visualizing Age Distribution"""

# Analyse Age distribution
Emp_Perf["Age"].plot.hist()

"""## Visualizing Promotions Trends"""

# Analyse Years since Promotion
Emp_Perf["YearsSinceLastPromotion"].plot.hist()

"""### From above plot it can be inferred that a whole lot of employees were being promoted quite often i.e. in 0–1.5 years."""

Emp_Perf.info()

#Missing Values
Emp_Perf.isnull().values.any()

#Check Target Variable values
Emp_Perf["PerformanceRating"].unique()

"""# Split numeric & categorical data and change categorical data with Label Encoding

# Categorical Data
"""

#All the Categorical Features

cat_col = Emp_Perf[['EmpNumber','Gender', 'EducationBackground', 'MaritalStatus', 'EmpDepartment', 'EmpJobRole', 'BusinessTravelFrequency', 'OverTime','Attrition']]
cat_col.head()

"""# Visualizing Attrition Count in Dataset"""

sns.set(rc={'figure.figsize':(7,7)})
sns.countplot(x="Attrition", data=cat_col)

"""# Printing all the Categorical Variable Datatypes and their Unique Values"""

for column in cat_col.columns:
    if cat_col[column].dtype == object:
        print(str(column) + ':' + str(cat_col[column].unique()))
        print(cat_col[column].value_counts())
        print('______________________________________________________________________________________')

"""# Numerical Data"""

#Numerical Data

num_data= Emp_Perf[Emp_Perf.columns[~Emp_Perf.columns.isin(cat_col)]]
num_data.head()

"""# Label Encoding"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

#cat_col = le.fit_transform(cat_col,axis=1)
cat_col_data=cat_col.apply(le.fit_transform)

"""#  Label Encoded Data"""

cat_col_data.head()

"""# Merging the Label Encoded Data and Numeric Data"""

Emp_Perf1=pd.concat([cat_col_data,num_data],axis=1)
Emp_Perf1.head(10)

Emp_Perf1.shape

""" # Setting up Target and Predictor Variables"""

col = list(Emp_Perf1)
predictors = Emp_Perf1[col[1:27]]
target = Emp_Perf1['PerformanceRating']

predictors.head()

target.head(5)

"""# Univarate Analysis

Statistical tests can be used to select those features that have the strongest relationship with the output variable.

The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.

The example below uses the chi-squared (chi²) statistical test for non-negative features to select 10 of the best features.
"""

#apply SelectKBest class to extract top 10 best features
from sklearn.datasets import load_digits
from sklearn.feature_selection import SelectKBest, chi2
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(predictors,target)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(predictors.columns)

#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Features','Score']  #naming the dataframe columns

#printing the scores of each features 
featureScores

#print 10 best features
print(featureScores.nlargest(10,'Score'))

"""# Feature Importance

You can get the feature importance of each feature of your dataset by using the feature importance property of the model.

Feature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.

Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.
"""

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(predictors,target)

#use inbuilt class feature_importances of tree based classifiers
print(model.feature_importances_)

#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=predictors.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

# Concatenation of Numeric Data (Non - Categorical Data)
df = Emp_Perf1[['EmpLastSalaryHikePercent','EmpEnvironmentSatisfaction', 'YearsSinceLastPromotion', 'EmpDepartment', 'ExperienceYearsInCurrentRole', 'EmpJobRole', 'YearsWithCurrManager', 'DistanceFromHome','ExperienceYearsAtThisCompany','NumCompaniesWorked','PerformanceRating']]
df.head()

"""# Modelling"""

### Independent and Dependent features
X=df.iloc[:,:-1]
y=df.iloc[:,-1]

X.head(5)

y.head()

"""# Splitting the Dataset into Training and Test sets"""

### Train Test Split
from sklearn.model_selection import train_test_split

# Splitting the Data into Training and Test set with test size of 0.2
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)

"""# Applying Random Forest Classifier"""

### Implement Random Forest classifier
from sklearn.ensemble import RandomForestClassifier
classifier1=RandomForestClassifier()
classifier1.fit(X_train,y_train)

"""# Prediction of Test Cases using Random Forest"""

## Prediction
y_pred1=classifier1.predict(X_test)

"""# Applying Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
classifier2 = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier2.fit(X_train, y_train)

"""# Prediction of Test Cases using Decision Tree"""

## Prediction
y_pred2=classifier2.predict(X_test)

"""#  Applying LightGBM Classifier"""

#LGBM
import lightgbm as lgb
classifier4 = lgb.LGBMClassifier()
classifier4.fit(X_train, y_train)

"""# Prediction of Test Cases using LightGBM"""

y_pred4=classifier4.predict(X_test)

"""#  Applying XGBoost Classifier"""

# Applying XG Boost Classifier
from xgboost import XGBClassifier
classifier3 = XGBClassifier()
classifier3.fit(X_train, y_train)

"""# Prediction of Test Cases using XGBoost"""

## Prediction
y_pred3=classifier3.predict(X_test)

"""# Model Accuracy and Results for Random Forest Classifier"""

#Confusion matrix in order to predict actual and predicted values
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test,y_pred1)
print(cm)
print(classification_report(y_test,y_pred1))

"""# Model Accuracy and Results for Decision Tree Classifier"""

#Confusion matrix in order to predict actual and predicted values
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test,y_pred2)
print(cm)
print(classification_report(y_test,y_pred2))

"""# Model Accuracy and Results for XGBoost Classifier"""

#Confusion matrix in order to predict actual and predicted values - XGBoost
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test,y_pred3)
print(cm)
print(classification_report(y_test,y_pred3))

"""# Model Accuracy and Results for Light GBM Classifier"""

#Confusion matrix in order to predict actual and predicted values
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test,y_pred4)
print(cm)
print(classification_report(y_test,y_pred4))

"""# Applying 10-Fold Cross Validation on Random Forest Classifier"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier1, X = X_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""# Inference

There are very few mis -classifications and the precision rates for all the values are very high as is seen from the snapshot here and it implies that our model is highly accurate.

# Creating a Pickle file for Random Forest Classifier Deployment on Flask
"""

### Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier1.pkl","wb") #Pickle file created with name classfier.pkl and stored in the working directory
pickle.dump(classifier1, pickle_out)
pickle_out.close()

classifier1.predict([[12,4,0,5,7,13,8,10,10,1]]) # Testing model with input on 10 top Features

classifier2.predict([[12,4,0,5,7,13,8,10,10,1]]) # Testing model with input on 10 top Features

"""# Creating a Pickle file for Decision Tree Classifier Deployment on Flask"""

### Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier2.pkl","wb") #Pickle file created with name classfier.pkl and stored in the working directory
pickle.dump(classifier2, pickle_out)
pickle_out.close()

"""# Creating a Pickle file for XG Boost Classifier Deployment on Flask"""

### Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier3.pkl","wb") #Pickle file created with name classfier.pkl and stored in the working directory
pickle.dump(classifier3, pickle_out)
pickle_out.close()

"""# Creating a Pickle file for Light GBM Classifer Deployment on Flask"""

### Create a Pickle file using serialization 
import pickle
pickle_out = open("classifier4.pkl","wb") #Pickle file created with name classfier.pkl and stored in the working directory
pickle.dump(classifier4, pickle_out)
pickle_out.close()